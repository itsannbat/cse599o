W1204 20:21:27.895000 140624578317184 torch/distributed/run.py:779] 
W1204 20:21:27.895000 140624578317184 torch/distributed/run.py:779] *****************************************
W1204 20:21:27.895000 140624578317184 torch/distributed/run.py:779] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1204 20:21:27.895000 140624578317184 torch/distributed/run.py:779] *****************************************
[rank1]: Traceback (most recent call last):
[rank1]:   File "/homes/iws/abatur/cse599o/cse599o/project/fsdp_train_4gpu.py", line 175, in <module>
[rank1]:     main_worker()
[rank1]:   File "/homes/iws/abatur/cse599o/cse599o/project/fsdp_train_4gpu.py", line 150, in main_worker
[rank1]:     output = fsdp_model(input_ids)
[rank1]:              ^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/homes/iws/abatur/.venv/lib64/python3.12/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
[rank1]:     return self._call_impl(*args, **kwargs)
[rank1]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/homes/iws/abatur/.venv/lib64/python3.12/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
[rank1]:     return forward_call(*args, **kwargs)
[rank1]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/homes/iws/abatur/.venv/lib64/python3.12/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py", line 863, in forward
[rank1]:     output = self._fsdp_wrapped_module(*args, **kwargs)
[rank1]:              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/homes/iws/abatur/.venv/lib64/python3.12/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
[rank1]:     return self._call_impl(*args, **kwargs)
[rank1]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/homes/iws/abatur/.venv/lib64/python3.12/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
[rank1]:     return forward_call(*args, **kwargs)
[rank1]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/homes/iws/abatur/.venv/lib64/python3.12/site-packages/transformers/models/gpt2/modeling_gpt2.py", line 1068, in forward
[rank1]:     transformer_outputs = self.transformer(
[rank1]:                           ^^^^^^^^^^^^^^^^^
[rank1]:   File "/homes/iws/abatur/.venv/lib64/python3.12/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
[rank1]:     return self._call_impl(*args, **kwargs)
[rank1]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/homes/iws/abatur/.venv/lib64/python3.12/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
[rank1]:     return forward_call(*args, **kwargs)
[rank1]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/homes/iws/abatur/.venv/lib64/python3.12/site-packages/transformers/models/gpt2/modeling_gpt2.py", line 925, in forward
[rank1]:     outputs = block(
[rank1]:               ^^^^^^
[rank1]:   File "/homes/iws/abatur/.venv/lib64/python3.12/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
[rank1]:     return self._call_impl(*args, **kwargs)
[rank1]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/homes/iws/abatur/.venv/lib64/python3.12/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
[rank1]:     return forward_call(*args, **kwargs)
[rank1]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/homes/iws/abatur/.venv/lib64/python3.12/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py", line 863, in forward
[rank1]:     output = self._fsdp_wrapped_module(*args, **kwargs)
[rank1]:              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/homes/iws/abatur/.venv/lib64/python3.12/site-packages/transformers/modeling_layers.py", line 94, in __call__
[rank1]:     return super().__call__(*args, **kwargs)
[rank1]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/homes/iws/abatur/.venv/lib64/python3.12/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
[rank1]:     return self._call_impl(*args, **kwargs)
[rank1]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/homes/iws/abatur/.venv/lib64/python3.12/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
[rank1]:     return forward_call(*args, **kwargs)
[rank1]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/homes/iws/abatur/.venv/lib64/python3.12/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
[rank1]:     return func(*args, **kwargs)
[rank1]:            ^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/homes/iws/abatur/.venv/lib64/python3.12/site-packages/transformers/models/gpt2/modeling_gpt2.py", line 449, in forward
[rank1]:     feed_forward_hidden_states = self.mlp(hidden_states)
[rank1]:                                  ^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/homes/iws/abatur/.venv/lib64/python3.12/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
[rank1]:     return self._call_impl(*args, **kwargs)
[rank1]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/homes/iws/abatur/.venv/lib64/python3.12/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
[rank1]:     return forward_call(*args, **kwargs)
[rank1]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/homes/iws/abatur/.venv/lib64/python3.12/site-packages/transformers/models/gpt2/modeling_gpt2.py", line 375, in forward
[rank1]:     hidden_states = self.act(hidden_states)
[rank1]:                     ^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/homes/iws/abatur/.venv/lib64/python3.12/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
[rank1]:     return self._call_impl(*args, **kwargs)
[rank1]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/homes/iws/abatur/.venv/lib64/python3.12/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
[rank1]:     return forward_call(*args, **kwargs)
[rank1]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/homes/iws/abatur/.venv/lib64/python3.12/site-packages/transformers/activations.py", line 62, in forward
[rank1]:     return 0.5 * input * (1.0 + torch.tanh(math.sqrt(2.0 / math.pi) * (input + 0.044715 * torch.pow(input, 3.0))))
[rank1]:                                                                                           ^^^^^^^^^^^^^^^^^^^^^
[rank1]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 96.00 MiB. GPU 1 has a total capacity of 23.46 GiB of which 61.81 MiB is free. Including non-PyTorch memory, this process has 23.40 GiB memory in use. Of the allocated memory 23.02 GiB is allocated by PyTorch, and 55.70 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank0]: Traceback (most recent call last):
[rank0]:   File "/homes/iws/abatur/cse599o/cse599o/project/fsdp_train_4gpu.py", line 175, in <module>
[rank0]:     main_worker()
[rank0]:   File "/homes/iws/abatur/cse599o/cse599o/project/fsdp_train_4gpu.py", line 150, in main_worker
[rank0]:     output = fsdp_model(input_ids)
[rank0]:              ^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/homes/iws/abatur/.venv/lib64/python3.12/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/homes/iws/abatur/.venv/lib64/python3.12/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/homes/iws/abatur/.venv/lib64/python3.12/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py", line 863, in forward
[rank0]:     output = self._fsdp_wrapped_module(*args, **kwargs)
[rank0]:              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/homes/iws/abatur/.venv/lib64/python3.12/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/homes/iws/abatur/.venv/lib64/python3.12/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/homes/iws/abatur/.venv/lib64/python3.12/site-packages/transformers/models/gpt2/modeling_gpt2.py", line 1068, in forward
[rank0]:     transformer_outputs = self.transformer(
[rank0]:                           ^^^^^^^^^^^^^^^^^
[rank0]:   File "/homes/iws/abatur/.venv/lib64/python3.12/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/homes/iws/abatur/.venv/lib64/python3.12/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/homes/iws/abatur/.venv/lib64/python3.12/site-packages/transformers/models/gpt2/modeling_gpt2.py", line 925, in forward
[rank0]:     outputs = block(
[rank0]:               ^^^^^^
[rank0]:   File "/homes/iws/abatur/.venv/lib64/python3.12/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/homes/iws/abatur/.venv/lib64/python3.12/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/homes/iws/abatur/.venv/lib64/python3.12/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py", line 863, in forward
[rank0]:     output = self._fsdp_wrapped_module(*args, **kwargs)
[rank0]:              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/homes/iws/abatur/.venv/lib64/python3.12/site-packages/transformers/modeling_layers.py", line 94, in __call__
[rank0]:     return super().__call__(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/homes/iws/abatur/.venv/lib64/python3.12/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/homes/iws/abatur/.venv/lib64/python3.12/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/homes/iws/abatur/.venv/lib64/python3.12/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
[rank0]:     return func(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/homes/iws/abatur/.venv/lib64/python3.12/site-packages/transformers/models/gpt2/modeling_gpt2.py", line 449, in forward
[rank0]:     feed_forward_hidden_states = self.mlp(hidden_states)
[rank0]:                                  ^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/homes/iws/abatur/.venv/lib64/python3.12/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/homes/iws/abatur/.venv/lib64/python3.12/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/homes/iws/abatur/.venv/lib64/python3.12/site-packages/transformers/models/gpt2/modeling_gpt2.py", line 375, in forward
[rank0]:     hidden_states = self.act(hidden_states)
[rank0]:                     ^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/homes/iws/abatur/.venv/lib64/python3.12/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/homes/iws/abatur/.venv/lib64/python3.12/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/homes/iws/abatur/.venv/lib64/python3.12/site-packages/transformers/activations.py", line 62, in forward
[rank0]:     return 0.5 * input * (1.0 + torch.tanh(math.sqrt(2.0 / math.pi) * (input + 0.044715 * torch.pow(input, 3.0))))
[rank0]:                                                                                           ^^^^^^^^^^^^^^^^^^^^^
[rank0]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 96.00 MiB. GPU 0 has a total capacity of 23.46 GiB of which 65.81 MiB is free. Including non-PyTorch memory, this process has 23.39 GiB memory in use. Of the allocated memory 23.02 GiB is allocated by PyTorch, and 55.70 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank3]: Traceback (most recent call last):
[rank3]:   File "/homes/iws/abatur/cse599o/cse599o/project/fsdp_train_4gpu.py", line 175, in <module>
[rank3]:     main_worker()
[rank3]:   File "/homes/iws/abatur/cse599o/cse599o/project/fsdp_train_4gpu.py", line 150, in main_worker
[rank3]:     output = fsdp_model(input_ids)
[rank3]:              ^^^^^^^^^^^^^^^^^^^^^
[rank3]:   File "/homes/iws/abatur/.venv/lib64/python3.12/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
[rank3]:     return self._call_impl(*args, **kwargs)
[rank3]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:   File "/homes/iws/abatur/.venv/lib64/python3.12/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
[rank3]:     return forward_call(*args, **kwargs)
[rank3]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:   File "/homes/iws/abatur/.venv/lib64/python3.12/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py", line 863, in forward
[rank3]:     output = self._fsdp_wrapped_module(*args, **kwargs)
[rank3]:              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:   File "/homes/iws/abatur/.venv/lib64/python3.12/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
[rank3]:     return self._call_impl(*args, **kwargs)
[rank3]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:   File "/homes/iws/abatur/.venv/lib64/python3.12/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
[rank3]:     return forward_call(*args, **kwargs)
[rank3]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:   File "/homes/iws/abatur/.venv/lib64/python3.12/site-packages/transformers/models/gpt2/modeling_gpt2.py", line 1068, in forward
[rank3]:     transformer_outputs = self.transformer(
[rank3]:                           ^^^^^^^^^^^^^^^^^
[rank3]:   File "/homes/iws/abatur/.venv/lib64/python3.12/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
[rank3]:     return self._call_impl(*args, **kwargs)
[rank3]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:   File "/homes/iws/abatur/.venv/lib64/python3.12/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
[rank3]:     return forward_call(*args, **kwargs)
[rank3]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:   File "/homes/iws/abatur/.venv/lib64/python3.12/site-packages/transformers/models/gpt2/modeling_gpt2.py", line 925, in forward
[rank3]:     outputs = block(
[rank3]:               ^^^^^^
[rank3]:   File "/homes/iws/abatur/.venv/lib64/python3.12/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
[rank3]:     return self._call_impl(*args, **kwargs)
[rank3]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:   File "/homes/iws/abatur/.venv/lib64/python3.12/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
[rank3]:     return forward_call(*args, **kwargs)
[rank3]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:   File "/homes/iws/abatur/.venv/lib64/python3.12/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py", line 863, in forward
[rank3]:     output = self._fsdp_wrapped_module(*args, **kwargs)
[rank3]:              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:   File "/homes/iws/abatur/.venv/lib64/python3.12/site-packages/transformers/modeling_layers.py", line 94, in __call__
[rank3]:     return super().__call__(*args, **kwargs)
[rank3]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:   File "/homes/iws/abatur/.venv/lib64/python3.12/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
[rank3]:     return self._call_impl(*args, **kwargs)
[rank3]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:   File "/homes/iws/abatur/.venv/lib64/python3.12/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
[rank3]:     return forward_call(*args, **kwargs)
[rank3]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:   File "/homes/iws/abatur/.venv/lib64/python3.12/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
[rank3]:     return func(*args, **kwargs)
[rank3]:            ^^^^^^^^^^^^^^^^^^^^^
[rank3]:   File "/homes/iws/abatur/.venv/lib64/python3.12/site-packages/transformers/models/gpt2/modeling_gpt2.py", line 449, in forward
[rank3]:     feed_forward_hidden_states = self.mlp(hidden_states)
[rank3]:                                  ^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:   File "/homes/iws/abatur/.venv/lib64/python3.12/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
[rank3]:     return self._call_impl(*args, **kwargs)
[rank3]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:   File "/homes/iws/abatur/.venv/lib64/python3.12/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
[rank3]:     return forward_call(*args, **kwargs)
[rank3]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:   File "/homes/iws/abatur/.venv/lib64/python3.12/site-packages/transformers/models/gpt2/modeling_gpt2.py", line 375, in forward
[rank3]:     hidden_states = self.act(hidden_states)
[rank3]:                     ^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:   File "/homes/iws/abatur/.venv/lib64/python3.12/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
[rank3]:     return self._call_impl(*args, **kwargs)
[rank3]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:   File "/homes/iws/abatur/.venv/lib64/python3.12/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
[rank3]:     return forward_call(*args, **kwargs)
[rank3]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank3]:   File "/homes/iws/abatur/.venv/lib64/python3.12/site-packages/transformers/activations.py", line 62, in forward
[rank3]:     return 0.5 * input * (1.0 + torch.tanh(math.sqrt(2.0 / math.pi) * (input + 0.044715 * torch.pow(input, 3.0))))
[rank3]:                                                                                           ^^^^^^^^^^^^^^^^^^^^^
[rank3]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 96.00 MiB. GPU 3 has a total capacity of 23.46 GiB of which 81.81 MiB is free. Including non-PyTorch memory, this process has 23.38 GiB memory in use. Of the allocated memory 23.02 GiB is allocated by PyTorch, and 55.70 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank2]: Traceback (most recent call last):
[rank2]:   File "/homes/iws/abatur/cse599o/cse599o/project/fsdp_train_4gpu.py", line 175, in <module>
[rank2]:     main_worker()
[rank2]:   File "/homes/iws/abatur/cse599o/cse599o/project/fsdp_train_4gpu.py", line 150, in main_worker
[rank2]:     output = fsdp_model(input_ids)
[rank2]:              ^^^^^^^^^^^^^^^^^^^^^
[rank2]:   File "/homes/iws/abatur/.venv/lib64/python3.12/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
[rank2]:     return self._call_impl(*args, **kwargs)
[rank2]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank2]:   File "/homes/iws/abatur/.venv/lib64/python3.12/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
[rank2]:     return forward_call(*args, **kwargs)
[rank2]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank2]:   File "/homes/iws/abatur/.venv/lib64/python3.12/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py", line 863, in forward
[rank2]:     output = self._fsdp_wrapped_module(*args, **kwargs)
[rank2]:              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank2]:   File "/homes/iws/abatur/.venv/lib64/python3.12/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
[rank2]:     return self._call_impl(*args, **kwargs)
[rank2]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank2]:   File "/homes/iws/abatur/.venv/lib64/python3.12/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
[rank2]:     return forward_call(*args, **kwargs)
[rank2]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank2]:   File "/homes/iws/abatur/.venv/lib64/python3.12/site-packages/transformers/models/gpt2/modeling_gpt2.py", line 1068, in forward
[rank2]:     transformer_outputs = self.transformer(
[rank2]:                           ^^^^^^^^^^^^^^^^^
[rank2]:   File "/homes/iws/abatur/.venv/lib64/python3.12/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
[rank2]:     return self._call_impl(*args, **kwargs)
[rank2]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank2]:   File "/homes/iws/abatur/.venv/lib64/python3.12/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
[rank2]:     return forward_call(*args, **kwargs)
[rank2]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank2]:   File "/homes/iws/abatur/.venv/lib64/python3.12/site-packages/transformers/models/gpt2/modeling_gpt2.py", line 925, in forward
[rank2]:     outputs = block(
[rank2]:               ^^^^^^
[rank2]:   File "/homes/iws/abatur/.venv/lib64/python3.12/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
[rank2]:     return self._call_impl(*args, **kwargs)
[rank2]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank2]:   File "/homes/iws/abatur/.venv/lib64/python3.12/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
[rank2]:     return forward_call(*args, **kwargs)
[rank2]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank2]:   File "/homes/iws/abatur/.venv/lib64/python3.12/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py", line 863, in forward
[rank2]:     output = self._fsdp_wrapped_module(*args, **kwargs)
[rank2]:              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank2]:   File "/homes/iws/abatur/.venv/lib64/python3.12/site-packages/transformers/modeling_layers.py", line 94, in __call__
[rank2]:     return super().__call__(*args, **kwargs)
[rank2]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank2]:   File "/homes/iws/abatur/.venv/lib64/python3.12/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
[rank2]:     return self._call_impl(*args, **kwargs)
[rank2]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank2]:   File "/homes/iws/abatur/.venv/lib64/python3.12/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
[rank2]:     return forward_call(*args, **kwargs)
[rank2]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank2]:   File "/homes/iws/abatur/.venv/lib64/python3.12/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
[rank2]:     return func(*args, **kwargs)
[rank2]:            ^^^^^^^^^^^^^^^^^^^^^
[rank2]:   File "/homes/iws/abatur/.venv/lib64/python3.12/site-packages/transformers/models/gpt2/modeling_gpt2.py", line 449, in forward
[rank2]:     feed_forward_hidden_states = self.mlp(hidden_states)
[rank2]:                                  ^^^^^^^^^^^^^^^^^^^^^^^
[rank2]:   File "/homes/iws/abatur/.venv/lib64/python3.12/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
[rank2]:     return self._call_impl(*args, **kwargs)
[rank2]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank2]:   File "/homes/iws/abatur/.venv/lib64/python3.12/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
[rank2]:     return forward_call(*args, **kwargs)
[rank2]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank2]:   File "/homes/iws/abatur/.venv/lib64/python3.12/site-packages/transformers/models/gpt2/modeling_gpt2.py", line 375, in forward
[rank2]:     hidden_states = self.act(hidden_states)
[rank2]:                     ^^^^^^^^^^^^^^^^^^^^^^^
[rank2]:   File "/homes/iws/abatur/.venv/lib64/python3.12/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
[rank2]:     return self._call_impl(*args, **kwargs)
[rank2]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank2]:   File "/homes/iws/abatur/.venv/lib64/python3.12/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
[rank2]:     return forward_call(*args, **kwargs)
[rank2]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank2]:   File "/homes/iws/abatur/.venv/lib64/python3.12/site-packages/transformers/activations.py", line 62, in forward
[rank2]:     return 0.5 * input * (1.0 + torch.tanh(math.sqrt(2.0 / math.pi) * (input + 0.044715 * torch.pow(input, 3.0))))
[rank2]:                                                                                           ^^^^^^^^^^^^^^^^^^^^^
[rank2]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 96.00 MiB. GPU 2 has a total capacity of 23.46 GiB of which 61.81 MiB is free. Including non-PyTorch memory, this process has 23.40 GiB memory in use. Of the allocated memory 23.02 GiB is allocated by PyTorch, and 55.70 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
W1204 20:21:54.388000 140624578317184 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 1254671 closing signal SIGTERM
W1204 20:21:54.388000 140624578317184 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 1254672 closing signal SIGTERM
W1204 20:21:54.388000 140624578317184 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 1254673 closing signal SIGTERM
E1204 20:21:54.702000 140624578317184 torch/distributed/elastic/multiprocessing/api.py:833] failed (exitcode: 1) local_rank: 0 (pid: 1254670) of binary: /homes/iws/abatur/.venv/bin/python3
Traceback (most recent call last):
  File "/homes/iws/abatur/.venv/bin/torchrun", line 10, in <module>
    sys.exit(main())
             ^^^^^^
  File "/homes/iws/abatur/.venv/lib64/python3.12/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 348, in wrapper
    return f(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^
  File "/homes/iws/abatur/.venv/lib64/python3.12/site-packages/torch/distributed/run.py", line 901, in main
    run(args)
  File "/homes/iws/abatur/.venv/lib64/python3.12/site-packages/torch/distributed/run.py", line 892, in run
    elastic_launch(
  File "/homes/iws/abatur/.venv/lib64/python3.12/site-packages/torch/distributed/launcher/api.py", line 133, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/homes/iws/abatur/.venv/lib64/python3.12/site-packages/torch/distributed/launcher/api.py", line 264, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
/homes/iws/abatur/cse599o/cse599o/project/fsdp_train_4gpu.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-12-04_20:21:54
  host      : tempura.cs.washington.edu
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 1254670)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
